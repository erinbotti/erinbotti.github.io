<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MIND MELD | Erin Botti </title> <meta name="author" content="Erin Botti"> <meta name="description" content="A machine learning framework for robots to learn from suboptimal human demonstrators"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://erinbotti.github.io/projects/1_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Erin</span> Botti </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">MIND MELD</h1> <p class="post-description">A machine learning framework for robots to learn from suboptimal human demonstrators</p> </header> <article> <div class="m-4 d-flex"> <a href="https://ieeexplore.ieee.org/abstract/document/9889616" class="btn btn-project" rel="external nofollow noopener" target="_blank">Paper: HRI 2022</a> <a href="#References" class="btn btn-project">Citation</a> <a href="https://github.com/CORE-Robotics-Lab/MINDMELD" class="btn btn-project" rel="external nofollow noopener" target="_blank">Github</a> </div> <h3> Motivation and Background </h3> <p>Let’s envision a future where humans and robots are working together out in the real-world, for example, assistive robots in the home. This is a difficult problem because the real-world can be unstructured and unpredictable (e.g., the floor of my house is covered in dog toys that my robot vaccuum regularly gets stuck on). Ideally, robots would be pre-programmed to know how to act in any situation; however, it is impossible for programmers to predict every situation that a robot may encounter. Additionally, people have different preferences and needs. One person might want their cups put away in the cabinet right-side-up, whereas someone else may prefer cups to be upside-down. Therefore, the end-user should be able to communicate their needs and preferences to a robot, without needing programming experience.</p> <p>One method that enables non-expert users to teach robots new skills is Learning from Demonstration (LfD). With LfD, the person shows the robot how they want the task done, and the robot learns from the person’s demonstration. There are two main types of LfD: Human-centric and Robot-centric LfD. Human-centric LfD is the easiest for humans because the human performs the task, while the robot observes. For example, to teach an autonomous vehicle the correct way to drive, the person would simply drive the car. However, the robot only sees correct examples of how to do the task and does not learn how to recover from mistakes. To solve this problem, in Robot-centric LfD, the robot attempts the task and the human provides corrective feedback to the robot. In the autonomous vehicle example, the vehicle would drive, while the person moves the steering wheel to teach the robot what it should be doing instead. If the robot gets off course, the person can show the robot how to get back on track. The problem with Robot-centric LfD is that people tend to provide suboptimal feedback, due to fatigue, humans not understanding how the robot learns, etc. Incorrect feedback makes it harder for the robot to learn effectively. Also, people provide heterogenous feedback, meaning that people have different teaching strategies. In the autonomous vehicle example, when the robot is making a wrong turn, some demonstrators may turn the wheel all the way in the other direction (over-correcting), whereas others may only turn the wheel a small amount (under-correcting). In this project, we take advantage of the benefits of Robot-centric LfD, and mitigate the problems by creating a method that can learn from suboptimal and heterogenous demonstrators.</p> <h3> MIND MELD </h3> <p>Our method is called Mutual Information-driven Meta-learning from Demonstration (MIND MELD) <a class="citation" href="#MINDMELD">(Schrum et al., 2022)</a>.</p> <h5> Overview </h5> <p>MIND MELD works by first using a person’s corrective demonstrations to learn how they are teaching suboptimally. In the driving domain, the person provides demonstrations by moving the steering wheel. In this case, the ways they can be suboptimal are over/under-correcting (moving the steering wheel too much or too little) or being anticipatory/delayed (moving the wheel too early or too late). MIND MELD learns a continous embedding that describes the person’s suboptimal behavior. Then, MIND MELD utilizes this embedding to shift the person’s corrective demonstrations closer to an optimal demonstration.</p> <div class="row mt-3"> <div class="mx-auto mt-3 mt-md-0" text-align="center"> <figure> <video src="/assets/video/MINDMELD_overview.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" loop="" muted=""></video> </figure> </div> </div> <div class="caption"> MIND MELD learns how the person is teaching suboptimally and improves the human's labels. </div> <h5> Optimal Labels and Calibration Tasks</h5> <p>How do we define optimal? In this case, an optimal corrective label steers the car in the shortest path to the goal, while avoiding obstacles.</p> <p>In order to learn a person’s suboptimal teaching style, we need to compare their corrective labels to optimal labels. Therefore, we calculate optimal labels for a small set of calibration tasks. These calibration tasks are a simple representative subset of tasks. In the driving domain, examples of calibration tasks are the car turning left when it should have gone right or going straight and hitting an obstacle when it should have turned left around the obstacle.</p> <p>With a more complicated robot task, we envision the person needing to teach the robot a few simple tasks with a block (e.g., pick and place, pushing the block, etc.) to calibrate the robot, meaning the robot learns the person’s suboptimal embedding. Then the person could move on to teaching the robot house cleaning tasks, and the robot could correct for the person’s suboptimal teaching style.</p> <p><strong>Note: We only need ground truth labels for the calibration tasks. We do not know optimal labels for test tasks.</strong></p> <h5> Network Architecture </h5> <p>The MIND MELD network architecture is in the figure below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MINDMELD_Network.png" sizes="95vw"></source> <img src="/assets/img/MINDMELD_Network.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="MIND MELD Network Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> MIND MELD Architecture </div> <p>The MIND MELD architecture consists of three parts: 1) the bi-directional Long Short-Term Memory (LSTM) encoder, \(\mathcal{E}_{\phi'}\), 2) the prediction subnetwork, \(f_\theta\), and 3) the recreation subnetwork, \(q_\phi\). The goal of the prediction subnetwork, \(f_\theta\), is to use the demonstrator \(p\)’s corrective feedback, \(a_t^{(p)}\), and the demonstrator’s suboptimal embedding, \(w^{(p)}\), to learn the difference between the demonstrator’s feedback and optimal, \(\hat{d}_{t}^{(p)}\). Additionally, the goal of the recreation subnetwork, \(q_\phi\) is to learn the personalized embedding, \(w^{(p)}\), from the demonstrator labels and how far their labels are from optimal.</p> <h6> Training Phases:</h6> <ol> <li> <em>Calibration Phase:</em> A set of users complete the calibration tasks, where we know the ground truth labels. We collect a dataset to learn the MIND MELD parameters, \(\theta, \phi, \phi'\). The parameters are then frozen for the testing phase.</li> <li> <em>Testing Phase:</em> A new user completes the calibration tasks to determine their personalized embedding, \(w^{(p)}\). Using \(w^{(p)}\), we calculate how much their demonstration should be shifted, \(\hat{d}_{t}^{(p)}\), on a new task (where optimal is unknown).</li> </ol> <p><strong>Bi-directional LSTM, \(\mathcal{E}_{\phi'}\):</strong> The input, \(a_t^{(p)}\) is demonstrator \(p\) ‘s corrective label at time \(t\). The \(\Delta t\) labels before and after time \(t\) are passed into the bi-directional LSTM encoder. We use a bi-directional LSTM because the labels are not independent through time, meaning that the labels before and after the current label can provide information about the person’s teaching style. The bi-directional LSTM encodes the timeseries input into an encoding, \(z^{(p)}_{t-\Delta t: t+\Delta t}\).</p> <p><strong>Prediction Subnetwork, \(f_\theta\) :</strong> The encoded demonstrator feedback, \(z^{(p)}_{t-\Delta t: t+\Delta t}\), and the personalized embedding describing the person’s suboptimal teaching style, \(w^{(p)}\), is fed through the prediction subnetwork, \(f_\theta\). The prediction subnetwork then outputs \(\hat{d}_{t}^{(p)}\), which is the amount that the person’s corrective demonstration should be shifted to be closer to optimal. We train this subnetwork using the calibration tasks where we have access to optimal labels, \(o_t\). We utilize a mean-squared error (MSE) loss between the predicted difference, \(\hat{d}_{t}^{(p)}\), and the true difference, \(d_{t}^{(p)}\), where \(d_{t}^{(p)} = a_t^{(p)} - o_t\).</p> <p>The recreation subnetwork, \(q_\phi\), learns the estimate for the personalized embedding, \(\hat{w}^{(p)}\), and \(w^{(p)}\) is also an input to the architecture. We use the calibration tasks to estimate a user’s embedding, \(\hat{w}^{(p)}\). When we are determining a user’s embedding, we intialize \(w^{(p)}\) to the average embedding from all previous users. The input is updated through backpropagation during each round of training. After the user has completed the calibration tasks and is moving on to teaching the robot a new task, we use the estimated embedding, \(\hat{w}^{(p)}\), as the input.</p> <p><strong>Recreation Subnetwork, \(q_\phi\) :</strong> The recreation subnetwork, \(q_\phi\), takes the encoded demonstrator feedback, \(z^{(p)}_{t-\Delta t: t+\Delta t}\), and the amount that the person is suboptimal, \(\hat{d}_{t}^{(p)}\), to estimate the embedding that describes the person’s suboptimal teaching style, \(\hat{w}^{(p)}\). To estimate \(w^{(p)}\), we utilize variational inference by maximizing the mutual information between \(\hat{d}_{t}^{(p)}\), \(w^{(p)}\), and \(z^{(p)}_{t-\Delta t: t+\Delta t}\). We train this subnetwork using an MSE loss between the estimated embedding, \(\hat{w}^{(p)}\), and the previous guess, \(w^{(p)}\). The intuition here is that the more we learn about how to correct the person’s feedback (as our estimate for \(\hat{d}_{t}^{(p)}\) becomes more accurate), we can be more certain of our estimate for \(w^{(p)}\). Please see the <a href="https://ieeexplore.ieee.org/abstract/document/9889616" rel="external nofollow noopener" target="_blank">paper</a> for more details and equations.</p> <h3> Experiment and Selected Results </h3> <p>To evaluate MIND MELD, we conduct a human-subjects experiment. In the study, the robot is an autonomous vehicle in a driving simulator, and participants teach the car to drive to a goal location without hitting any obstacles.</p> <div class="row"> <div class="col-sm-1 mt-3 mt-md-0"> </div> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MINDMELD_simulator.png" sizes="95vw"></source> <img src="/assets/img/MINDMELD_simulator.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="MIND MELD Study Setup" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-1 mt-3 mt-md-0"> </div> </div> <div class="caption"> Participants teach a car in a driving simulator to drive using a physical steering wheel. The goal is to navigate the car to the large orange ball, without hitting any obstacles. </div> <p>For the <em>Calibration Phase</em>, where we train the MIND MELD architecture parameters, we recruited 76 participants. In this phase, participants completed the calibration tasks, where we have the optimal labels.</p> <p>To evaluate MIND MELD, we conduct a <em>Testing Phase</em>. In the testing phase, participants first completed the calibration tasks so we could learn their personalized embedding, \(w^{(p)}\). Then participants teach the car to drive on a new, more complicated task (increased number of obstacles and turns needed to reach the goal). Participants teach the car to drive the new task for three different algorithms (in a random order): MIND MELD, a robot-centric baseline (DAgger), and a human-centric baseline (Behavioral Cloning (BC)). For each algorithm, participants teach the car for twelve trials. We recruited 42 participants for this phase.</p> <h5> Metrics </h5> <p>To compare the accuracy of MIND MELD to the baselines, we measure the average distance to the goal and the probability of reaching the goal for each trial. Participants also complete surveys to measure the amount of perceived workload to teach using each algorithm and the likeability, perceived intelligence, and trust of each algorithm.</p> <p>Overall, we found that MIND MELD outperformed both baselines in terms of accuracy and participant perceptions!</p> <h5> Average Distance from Goal </h5> <div class="row"> <div class="col-sm-2 mt-3 mt-md-0"> </div> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MINDMELD_distance.png" sizes="95vw"></source> <img src="/assets/img/MINDMELD_distance.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="MIND MELD Average Distance" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-2 mt-3 mt-md-0"> </div> </div> <p>MIND MELD gets significantly closer to the goal compared to both baselines.</p> <h5> Probability of Reaching the Goal </h5> <div class="row"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MINDMELD_1x.png" sizes="95vw"></source> <img src="/assets/img/MINDMELD_1x.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="MIND MELD 1x" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MINDMELD_2x.png" sizes="95vw"></source> <img src="/assets/img/MINDMELD_2x.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="MIND MELD 2x" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MINDMELD_3x.png" sizes="95vw"></source> <img src="/assets/img/MINDMELD_3x.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="MIND MELD 3x" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MINDMELD_4x.png" sizes="95vw"></source> <img src="/assets/img/MINDMELD_4x.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="MIND MELD 4x" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In these plots, we compare the probability of each algorithm reaching the goal once (top left), twice (top right), three times (bottom left), and four times (bottom right) for each trial. Overall, MIND MELD has a significantly higher probability of reaching the goal compared to baselines. In the bottom right plot, MIND MELD has a 100% chance of reaching the goal four times during the trials, while both baselines have less than a 50% chance of achieving the same performance.</p> <h5> Subjective Metrics </h5> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MINDMELD_subjective.png" sizes="95vw"></source> <img src="/assets/img/MINDMELD_subjective.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="MIND MELD Subjective" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Participants rated MIND MELD has significantly less work to teach compared to baselines. Participants also perceived MIND MELD as more likeable and intelligent and trusted MIND MELD more compared to baselines.</p> </article> <h2 id="References">References</h2> <div class="publications"> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">HRI ’22</abbr> </div> <div id="MINDMELD" class="col-sm-8"> <div class="title">MIND MELD: Personalized Meta-Learning for Robot-Centric Imitation Learning</div> <div class="author"> Mariah Schrum, Erin Hedlund-Botti, Nina Moorman, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Matthew Gombolay' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</em>, Mar 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Learning from demonstration (LfD) techniques seek to enable users without computer programming experience to teach robots novel tasks. There are generally two types of LfD: human- and robot-centric. While human-centric learning is intuitive, human centric learning suffers from performance degradation due to covariate shift. Robot-centric approaches, such as Dataset Aggregation (DAgger), address covariate shift but can struggle to learn from suboptimal human teachers. To create a more human-aware version of robot-centric LfD, we present Mutual Information-driven Meta-learning from Demonstration (MIND MELD). MIND MELD meta-learns a mapping from suboptimal and heterogeneous human feedback to optimal labels, thereby improving the learning signal for robot-centric LfD. The key to our approach is learning an informative personalized em-bedding using mutual information maximization via variational inference. The embedding then informs a mapping from human provided labels to optimal labels. We evaluate our framework in a human-subjects experiment, demonstrating that our approach improves corrective labels provided by human demonstrators. Our framework outperforms baselines in terms of ability to reach the goal (p&lt;.001), average distance from the goal (p=.006), and various subjective ratings (p=.008).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">MINDMELD</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Schrum, Mariah and Hedlund-Botti, Erin and Moorman, Nina and Gombolay, Matthew}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MIND MELD: Personalized Meta-Learning for Robot-Centric Imitation Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{157-165}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/HRI53351.2022.9889616}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Erin Botti. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"Here is a sample of the research I conducted during my PhD.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"news-presented-our-paper-reciprocal-mind-meld-improving-learning-from-demonstration-via-personalized-reciprocal-teaching-at-corl-2022",title:"Presented our paper Reciprocal MIND MELD: Improving Learning From Demonstration via Personalized, Reciprocal...",description:"",section:"News"},{id:"news-presented-at-hri-pioneers-2023-workshop-read-my-paper-here",title:"Presented at HRI Pioneers 2023 workshop! Read my paper here.",description:"",section:"News"},{id:"news-received-the-peo-scholar-award-georgia-tech-press-release-here",title:"Received the PEO Scholar Award! Georgia Tech press release here.",description:"",section:"News"},{id:"news-our-paper-investigating-the-impact-of-experience-on-a-user-s-ability-to-perform-hierarchical-abstraction-was-nominated-for-best-student-paper-at-rss-2023",title:"Our paper Investigating the Impact of Experience on a User\u2019s Ability to Perform...",description:"",section:"News"},{id:"news-presented-at-the-robo-shop-workshop-for-ted-women-2023",title:"Presented at the Robo-shop workshop for TED Women 2023.",description:"",section:"News"},{id:"news-presented-our-workshop-paper-towards-learning-interpretable-features-from-interventions-at-the-lifelong-learning-and-personalization-in-long-term-human-robot-interaction-leap-hri-workshop-at-hri-2024",title:"Presented our workshop paper Towards Learning Interpretable Features from Interventions at the Lifelong...",description:"",section:"News"},{id:"news-i-defended-my-phd-thesis-improving-learning-from-demonstration-in-real-world-scenarios",title:"I defended my PhD Thesis: Improving Learning from Demonstration in Real-World Scenarios!",description:"",section:"News"},{id:"news-presented-our-paper-developing-design-guidelines-for-older-adults-with-robot-learning-from-demonstration-at-rss-2024",title:"Presented our paper Developing Design Guidelines for Older Adults with Robot Learning from...",description:"",section:"News"},{id:"projects-mind-meld",title:"MIND MELD",description:"A machine learning framework for robots to learn from suboptimal human demonstrators",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%65%64%6C%75%6E%64.%65%72%69%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=ScBKikgAAAAJ","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/erin-hedlund-botti","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>